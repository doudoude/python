knn 即可用于clasification,也用于regression
  knn(3)classification：取querry x最近的三个训练集x，找到对应的三个y,出现次数最多的y即为querry x的预测y值
  knn(3)regression:取querry x最近的三个训练集x，找到对应的三个y，取y的平均，即为querry x的预测y值
  选取合适的k，k过大：underfitting,k过小，overfitting，选择使测试集R方最大的k
linear regression假设y与x存在线性关系，在训练集上的R方不如KNN，但是测试集上优于knn。knn容易受极端训练值影响，而linear regression比较稳定
  linear regression的参数（斜率和截距）估计方法有四种：least square estimation,最小化实际值和预测值差的平和
  ridge在lse基础上加了一个惩罚项，惩罚过多/过大的feature系数w的平方(regularization).alpha越大，regularization越强,模型越简单，feature weights close to 0
  lasso类似与ridge，但是惩罚项为w的绝对值，会直接令不显著的系数为0
  polynomial regression:把x1,x2重新组合成五个特征：x1,x2,x1x2,x1方，x2方，对五个做回归。好处：把直线回归转换成曲线回归，capture non linear features
  但polynomial 会增大复杂度，因此需要与regularization模型(ridge)配合使用,即regularized polynomialregression
  logistic regression:对于逻辑回归和向量机，大的惩罚系数alpha意味着less regularization
ssupport vector model:用于sparse data,用一条线把两类分类，或多条线把 是/否 分开。maximizing the margin between classes,margin指类之间的宽度。
  参数c为regularization，c越大，regularization越小，对训练集数据更敏感，模型拟合更好，复杂度更高,但smaller margin
  c越小，模型更简单，错误率更高，larger margin
  kernel funvtion把二维数据转化为三维，再进行划分。划分函数称为kernel function,如radial basis function,这个函数的gamma参数表示单点的影响范围
  gamma小，相似范围大，模型平滑，更多的点被分到一起。 gamma大，模型更准确，但可能过拟合
cross validation把数据分成k份，选一份为测试集。注意划分的target要均匀，不能一份都是苹果，下一份都是梨
decision tree原理：依据特征重要程度依次划分训练集数据
  参数： max depth controls the maximum depth or the number of split points the decision can have and it's probably the most common 
  parameter used to reduce tree complexity and thus reduce overfitting. 
  The min samples leaf parameter is the threshold that controls what the minimum number of data instances has to be in a leaf to avoid
  splitting it further. This setting also reduces tree complexity. 
  Finally, max leaf nodes limits the total of nodes that are leaves of the tree. In effect, setting this parameter will indirectly
  influence the other two parameters and vice versa. 
